{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods for minimization with linear equality constraints\n",
    "- Newton method requiring feasible start point\n",
    "- Newton method allowing infeasible start point\n",
    "\n",
    "### Methods for minimization with general inequality constraints\n",
    "- Barrier method (iterated) using BFGS or Newton method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import grad as auto_grad, jacobian as auto_jacobian\n",
    "import scipy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_square_with_qr(A, b):\n",
    "    \"\"\"Solve system of equations with square (real values) matrix A: A * x = b\"\"\"\"\"\n",
    "    Q, R = np.linalg.qr(A)\n",
    "    x = scipy.linalg.solve_triangular(R, Q.T @ b)\n",
    "    return x\n",
    "\n",
    "\n",
    "def solve_symm_with_chol(A, b):\n",
    "    \"\"\"Solve system of equations with symmetric (hermitian) matrix A: A * x = b\"\"\"\"\"\n",
    "    c, lower = scipy.linalg.cho_factor(A)\n",
    "    x = scipy.linalg.cho_solve((c, lower), b)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n0 = 1\n",
    "def objective_fn0(x):\n",
    "    y = - x[..., 0]**2\n",
    "    return y\n",
    "\n",
    "def eq_constraint_fn0(x):\n",
    "    c0 = x[..., 0] - 1\n",
    "    return np.array([c0])\n",
    "\n",
    "grad_fn0 = auto_grad(objective_fn0)\n",
    "hessian_fn0 = auto_jacobian(grad_fn0)\n",
    "jacobian_eq_cnst0 = auto_jacobian(eq_constraint_fn0)\n",
    "\n",
    "n1 = 2\n",
    "def objective_fn1(x):\n",
    "    y = x[..., 0]**2 + x[..., 1]**2\n",
    "    return y\n",
    "\n",
    "def eq_constraint_fn1(x):\n",
    "#     c0 = x[..., 0]**2 + x[..., 1]**2 - 1\n",
    "#     return np.array([c0])\n",
    "#     c0 = x[..., 0]**2 - x[..., 1] - 1\n",
    "#     return np.array([c0])\n",
    "    c0 = 3*x[..., 0] - x[..., 1] - 1\n",
    "    return np.array([c0])\n",
    "\n",
    "grad_fn1 = auto_grad(objective_fn1)\n",
    "hessian_fn1 = auto_jacobian(grad_fn1)\n",
    "jacobian_eq_cnst1 = auto_jacobian(eq_constraint_fn1)\n",
    "\n",
    "\n",
    "\"\"\"Objectives with inequality constraints\"\"\"\n",
    "\n",
    "n1 = 1\n",
    "def objective_fn2(x):\n",
    "    y = - x[..., 0]**2\n",
    "    return y\n",
    "\n",
    "# Constraints as c <= 0\n",
    "def constraint2_1(x):\n",
    "    c = x[..., 0]**2 - 1\n",
    "#     c = np.sqrt((x[..., 0] - 1)**2) - 0.5\n",
    "    return np.array([c])\n",
    "\n",
    "grad_fn2 = auto_grad(objective_fn2)\n",
    "hessian_fn2 = auto_jacobian(grad_fn2)\n",
    "constraints2 = [constraint2_1]\n",
    "grad_constraints2 = [auto_grad(constraint) for constraint in constraints2]\n",
    "hessian_constraints2 = [auto_jacobian(grad_cnst) for grad_cnst in grad_constraints2]\n",
    "\n",
    "\n",
    "n1 = 2\n",
    "def objective_fn3(x):\n",
    "    y = x[..., 0]**2 + x[..., 1]**2\n",
    "    return y\n",
    "\n",
    "\n",
    "# Constraints as c <= 0\n",
    "\n",
    "def constraint3_1(x):\n",
    "    c = x[..., 0] + 1\n",
    "    return np.array([c])\n",
    "\n",
    "def constraint3_2(x):\n",
    "    c = - x[..., 1] + 2\n",
    "    return np.array([c])\n",
    "\n",
    "grad_fn3 = auto_grad(objective_fn3)\n",
    "hessian_fn3 = auto_jacobian(grad_fn3)\n",
    "constraints3 = [constraint3_1, constraint3_2]\n",
    "grad_constraints3 = [auto_grad(constraint) for constraint in constraints3]\n",
    "hessian_constraints3 = [auto_jacobian(grad_cnst) for grad_cnst in grad_constraints3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_newton_method_eq_constraint_feasible(objective_fn, grad_fn, hessian_fn, constraint_fn, jacobian_cnst_fn, x0,\n",
    "                                             alpha_min=1e-6, alpha_max=1e6,\n",
    "                                             obj_tol=1e-15, cnst_tol=1e-15, max_iterations=10**6):\n",
    "    \"\"\"Minimize non-linear function with linear equality constraints.\n",
    "    The initial point 'x0' has to be feasible.\"\"\"\n",
    "\n",
    "    assert np.abs(constraint_fn(x0)) < cnst_tol, \\\n",
    "        \"The initial point has to be feasible ({} is above tolerance {})\".format(\n",
    "            np.abs(constraint_fn(x0)), cnst_tol)\n",
    "\n",
    "    num_vars = x0.shape[0]\n",
    "    num_cnst = constraint_fn(x0).shape[0]\n",
    "    if x0.ndim == 1:\n",
    "        x0 = x0[:, np.newaxis]\n",
    "\n",
    "    x = np.array(x0)\n",
    "    iteration = 0\n",
    "    dobj = float(\"inf\")\n",
    "    obj_value = objective_fn(x[:, 0])\n",
    "    while np.abs(dobj) > obj_tol and iteration < max_iterations:\n",
    "        prev_obj_value = obj_value\n",
    "        grad = grad_fn(x[:, 0])[:, np.newaxis]\n",
    "        jacobian_cnst = jacobian_cnst_fn(x[:, 0])\n",
    "        constraint = constraint_fn(x[:, 0])[:, np.newaxis]\n",
    "        hessian = hessian_fn(x[:, 0])\n",
    "\n",
    "        # Generate newton system\n",
    "        A_upper = np.block([[hessian, jacobian_cnst.T]])\n",
    "        A_lower = np.block([[jacobian_cnst, np.zeros((num_cnst, num_cnst))]])\n",
    "        A = np.concatenate((A_upper, A_lower), axis=0)\n",
    "        b = np.concatenate((-grad, np.zeros((num_cnst, 1))), axis=0)\n",
    "        # Solve for step direction.\n",
    "        # This is usually the main computational burden so should be\n",
    "        # an efficient method depending on structure of A.\n",
    "        y = np.linalg.solve(A, b)\n",
    "        step_dir = y[:num_vars, :]\n",
    "        # Lagrange variables\n",
    "        nu = y[num_vars:, :]\n",
    "\n",
    "        # Backtracking line-search\n",
    "        alpha = alpha_max\n",
    "        x_next = x\n",
    "        dobj = float(\"inf\")\n",
    "        while dobj >= 0 and alpha >= alpha_min:\n",
    "            step = alpha * step_dir\n",
    "            x_next = x + step\n",
    "            obj_value = objective_fn(x_next[:, 0])\n",
    "            dobj = obj_value - prev_obj_value\n",
    "            alpha *= 0.5\n",
    "        iteration += 1\n",
    "\n",
    "        if dobj >= 0:\n",
    "            break\n",
    "        x = x_next\n",
    "\n",
    "    obj_value = objective_fn(x[:, 0])\n",
    "    constraint_value = constraint_fn(x[:, 0])\n",
    "    info = {\n",
    "        \"obj_value\": obj_value,\n",
    "        \"constraint_value\": constraint_value,\n",
    "        \"iteration\": iteration,\n",
    "        \"max_iterations\": max_iterations,\n",
    "        \"nu\": nu,\n",
    "    }\n",
    "    x = x[:, 0]\n",
    "    return x, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_newton_method_eq_constraint(objective_fn, grad_fn, hessian_fn,\n",
    "                                    constraint_fn, jacobian_cnst_fn,\n",
    "                                    x0, nu0=None,\n",
    "                                    eps=1e-15, r_tol=1e-15, cnst_tol=1e-15, max_iterations=10**6):\n",
    "    num_vars = x0.shape[0]\n",
    "    num_cnst = constraint_fn(x0).shape[0]\n",
    "    if nu0 is None:\n",
    "        nu0 = np.zeros((num_cnst, 1))\n",
    "    alpha = 0.25\n",
    "    beta = 0.5\n",
    "    if x0.ndim == 1:\n",
    "        x0 = x0[:, np.newaxis]\n",
    "    if nu0.ndim == 1:\n",
    "        nu0 = nu0[:, np.newaxis]\n",
    "\n",
    "    x = np.array(x0)\n",
    "    nu = np.array(nu0)\n",
    "    iteration = 0\n",
    "    while iteration < max_iterations:\n",
    "        grad = grad_fn(x[:, 0])[:, np.newaxis]\n",
    "        jacobian_cnst = jacobian_cnst_fn(x[:, 0])\n",
    "        constraint = constraint_fn(x[:, 0])[:, np.newaxis]\n",
    "        hessian = hessian_fn(x[:, 0])\n",
    "\n",
    "        # Generate newton system\n",
    "        A_upper = np.block([[hessian, jacobian_cnst.T]])\n",
    "        A_lower = np.block([[jacobian_cnst, np.zeros((num_cnst, num_cnst))]])\n",
    "        A = np.concatenate((A_upper, A_lower), axis=0)\n",
    "        b = np.concatenate((-grad + jacobian_cnst.T @ nu, -constraint), axis=0)\n",
    "        # Solve for step direction.\n",
    "        # This is usually the main computational burden so should be\n",
    "        # an efficient method depending on structure of A.\n",
    "        dy = np.linalg.solve(A, b)\n",
    "        dx = dy[:num_vars, :]\n",
    "        dnu = dy[num_vars:, :]\n",
    "        print(\"dx:\", dx, \", dnu:\", dnu)\n",
    "\n",
    "        r = lambda x, nu: np.concatenate((grad_fn(x[:, 0])[:, np.newaxis] + jacobian_cnst_fn(x[:, 0]).T @ nu, constraint_fn(x[:, 0])[:, np.newaxis]), axis=0)\n",
    "\n",
    "        # Line search on newton decrement\n",
    "        t = 1.\n",
    "        r_value1 = r(x, nu)\n",
    "        r_norm1 = np.linalg.norm(r_value1)\n",
    "        while True:\n",
    "            r_value2 = r(x + t * dx, nu + t * dnu)\n",
    "            r_norm2 = np.linalg.norm(r_value2)\n",
    "            if r_norm2 <= (1 - alpha * t) * r_norm1:\n",
    "                break\n",
    "            t *= beta\n",
    "        x += t * dx\n",
    "        nu += t * dnu\n",
    "        iteration += 1\n",
    "\n",
    "        if r_norm2 <= r_tol and np.abs(constraint_fn(x[:, 0])) <= cnst_tol:\n",
    "            break\n",
    "        if np.linalg.norm(t * dx) <= eps and np.linalg.norm(t * dy) <= eps:\n",
    "            break\n",
    "\n",
    "\n",
    "    obj_value = objective_fn(x[:, 0])\n",
    "    constraint_value = constraint_fn(x[:, 0])\n",
    "    info = {\n",
    "        \"obj_value\": obj_value,\n",
    "        \"constraint_value\": constraint_value,\n",
    "        \"r_value\": r_value2,\n",
    "        \"iteration\": iteration,\n",
    "        \"max_iterations\": max_iterations,\n",
    "        \"t\": t,\n",
    "        \"nu\": nu[:, 0],\n",
    "    }\n",
    "    x = x[:, 0]\n",
    "    return x, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Minimize function #0 with newton method\n",
    "x0 = np.array([5.])\n",
    "max_iterations = 10\n",
    "x_min, info = min_newton_method_eq_constraint(objective_fn0, grad_fn0, hessian_fn0,\n",
    "                                              eq_constraint_fn0, jacobian_eq_cnst0, x0,\n",
    "                                              max_iterations=max_iterations)\n",
    "print(\"minimum x: {}\".format(x_min))\n",
    "print(\"objective value: {}\".format(info[\"obj_value\"]))\n",
    "print(\"constraint value: {}\".format(info[\"constraint_value\"]))\n",
    "# print(\"r_value: {}\".format(info[\"r_value\"]))\n",
    "print(\"iteration: {}\".format(info[\"iteration\"]))\n",
    "\n",
    "# Plot function and found minimum\n",
    "plt.figure()\n",
    "x = np.linspace(-np.abs(x_min), +np.abs(x_min), 100)[:, np.newaxis]\n",
    "plt.plot(x, objective_fn0(x))\n",
    "plt.plot([x_min], [objective_fn0(x_min)], 'rx')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Minimize function #1 with newton method\n",
    "x0 = np.array([2., 5.])\n",
    "x0 = np.array([2., 5000.])\n",
    "# x0 = np.array([2., 0.])\n",
    "print(\"Constraint value of initial point:\", eq_constraint_fn1(x0))\n",
    "max_iterations = 10\n",
    "x_min, info = min_newton_method_eq_constraint(\n",
    "    objective_fn1, grad_fn1, hessian_fn1,\n",
    "    eq_constraint_fn1, jacobian_eq_cnst1, x0,\n",
    "    max_iterations=max_iterations)\n",
    "print(\"minimum x: {}\".format(x_min))\n",
    "print(\"objective value: {}\".format(info[\"obj_value\"]))\n",
    "print(\"constraint value: {}\".format(info[\"constraint_value\"]))\n",
    "# print(\"r_value: {}\".format(info[\"r_value\"]))\n",
    "print(\"iteration: {}\".format(info[\"iteration\"]))\n",
    "\n",
    "# Plot function and found minimum\n",
    "plt.figure()\n",
    "x = np.linspace(-np.abs(x_min[0]), +np.abs(x_min[0]), 100)[:, np.newaxis]\n",
    "plt.plot(x, [objective_fn1(np.array([v, x_min[1]])) for v in x])\n",
    "plt.plot([x_min[0]], [objective_fn1(x_min)], 'rx')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "x = np.linspace(-np.abs(x_min[1]), +np.abs(x_min[1]), 100)[:, np.newaxis]\n",
    "plt.plot(x, [objective_fn1(np.array([x_min[0], v])) for v in x])\n",
    "plt.plot([x_min[1]], [objective_fn1(x_min)], 'rx')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimize function #1 with newton method starting from feasible point\n",
    "x0 = np.array([2., 5.])\n",
    "max_iterations = 100\n",
    "x_min, info = min_newton_method_eq_constraint_feasible(\n",
    "    objective_fn1, grad_fn1, hessian_fn1,\n",
    "    eq_constraint_fn1, jacobian_eq_cnst1,\n",
    "    x0, max_iterations=max_iterations)\n",
    "print(\"minimum x: {}\".format(x_min))\n",
    "print(\"objective value: {}\".format(info[\"obj_value\"]))\n",
    "print(\"constraint value: {}\".format(info[\"constraint_value\"]))\n",
    "print(\"iteration: {}\".format(info[\"iteration\"]))\n",
    "\n",
    "# Plot function and found minimum\n",
    "plt.figure()\n",
    "x = np.linspace(-np.abs(x_min[0]), +np.abs(x_min[0]), 100)[:, np.newaxis]\n",
    "plt.plot(x, [objective_fn1(np.array([v, x_min[1]])) for v in x])\n",
    "plt.plot([x_min[0]], [objective_fn1(x_min)], 'rx')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "x = np.linspace(-np.abs(x_min[1]), +np.abs(x_min[1]), 100)[:, np.newaxis]\n",
    "plt.plot(x, [objective_fn1(np.array([x_min[0], v])) for v in x])\n",
    "plt.plot([x_min[1]], [objective_fn1(x_min)], 'rx')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_newton_method(objective_fn, grad_fn, hessian_fn, x0,\n",
    "                      t_min=1e-6, eps=1e-15, obj_tol=1e-15, max_iterations=10**6,\n",
    "                      verbose=False):\n",
    "    x = np.array(x0)\n",
    "    t = 1.\n",
    "    iteration = 0\n",
    "    dobj = float(\"inf\")\n",
    "    obj_value = objective_fn(x)\n",
    "    print(obj_value, iteration, dobj, obj_tol)\n",
    "    while np.abs(dobj) > obj_tol and iteration < max_iterations:\n",
    "        if verbose:\n",
    "            print(\"iteration:\", iteration)\n",
    "            print(\"  x:\", x)\n",
    "        prev_obj_value = obj_value\n",
    "        grad = grad_fn(x)\n",
    "        hessian = hessian_fn(x)\n",
    "        if verbose:\n",
    "            print(\"  grad:\", grad)\n",
    "            print(\"  hessian:\", hessian)\n",
    "\n",
    "        # Compute step direction (+ simple method for Hessian modification)\n",
    "        hessian_mod_factor = 1\n",
    "        hessian_mod = hessian\n",
    "        positive_definite = False\n",
    "        while not positive_definite:\n",
    "            try:\n",
    "                # Cholesky factorization will fail if hessian not positive definite\n",
    "                step = solve_symm_with_chol(hessian_mod, -grad)\n",
    "                positive_definite = True\n",
    "            except np.linalg.LinAlgError as exc:\n",
    "                # In that case we add a multiple of the identity matrix\n",
    "                hessian_mod = hessian + hessian_mod_factor * np.eye(hessian.shape[0])\n",
    "                # And increase factor if still not positive definite\n",
    "                hessian_mod_factor *= 2\n",
    "        if verbose:\n",
    "            print(\"  hessian_mod:\", hessian_mod)\n",
    "            print(\"  step:\", step)\n",
    "\n",
    "        newton_decrement_square = - grad.T @ step\n",
    "        print(grad, step, newton_decrement_square)\n",
    "        if newton_decrement_square * 0.5 < eps:\n",
    "            if verbose:\n",
    "                print(\"Newton decrement below threshold\")\n",
    "            break\n",
    "\n",
    "        # Backtracking line search\n",
    "        t = 1.\n",
    "        x_next = x\n",
    "        dobj = float(\"inf\")\n",
    "        while dobj > 0 and t >= t_min:\n",
    "            x_next = x + t * step\n",
    "            obj_value = objective_fn(x_next)\n",
    "            dobj = obj_value - prev_obj_value\n",
    "            t *= 0.5\n",
    "        if verbose:\n",
    "            print(\"Line search stopped with t={}, x_next={}, dobj={}, obj_value={}\".format(\n",
    "                t, x_next, dobj, obj_value))\n",
    "        x = x_next\n",
    "        iteration += 1\n",
    "    info = {\n",
    "        \"obj_value\": obj_value,\n",
    "        \"dobj\": dobj,\n",
    "        \"obj_tol\": obj_tol,\n",
    "        \"iteration\": iteration,\n",
    "        \"max_iterations\": max_iterations,\n",
    "        \"grad\": grad,\n",
    "        \"hessian\": hessian,\n",
    "        \"hessian_mod\": hessian_mod,\n",
    "        \"t\": t,\n",
    "        \"step\": step,\n",
    "    }\n",
    "    return x, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_bfgs(objective_fn, grad_fn, x0,\n",
    "             t_min=1e-6, eps=1e-15, obj_tol=1e-15, beta=0.5, c=0.0001,\n",
    "             max_iterations=10**6, verbose=False):\n",
    "    x = np.array(x0)\n",
    "    # Initialize inverse Hessian approximation\n",
    "    B_inv = np.eye(x.shape[-1])\n",
    "    t = None\n",
    "    step = None\n",
    "    iteration = 0\n",
    "    dobj = float(\"inf\")\n",
    "    obj_value = objective_fn(x)\n",
    "    while np.abs(dobj) > obj_tol and iteration < max_iterations:\n",
    "        if verbose:\n",
    "            print(\"iteration:\", iteration)\n",
    "        prev_obj_value = obj_value\n",
    "        grad = grad_fn(x)\n",
    "        if verbose:\n",
    "            print(\"  grad:\", grad)\n",
    "            print(\"  B_inv:\", B_inv)\n",
    "\n",
    "        # Simple method for Hessian modification (faster methods available)\n",
    "        hessian_mod_factor = 1\n",
    "        B_inv_mod = B_inv\n",
    "        positive_definite = False\n",
    "        while not positive_definite:\n",
    "            try:\n",
    "                # Cholesky factorization will fail if hessian not positive definite\n",
    "                scipy.linalg.cho_factor(B_inv_mod)\n",
    "                positive_definite = True\n",
    "            except np.linalg.LinAlgError as exc:\n",
    "                # In that case we add a multiple of the identity matrix\n",
    "                B_inv_mod = B_inv + hessian_mod_factor * np.eye(B_inv.shape[0])\n",
    "                # And increase factor if still not positive definite\n",
    "                hessian_mod_factor *= 2\n",
    "\n",
    "        step_dir = -B_inv_mod @ grad\n",
    "        if verbose:\n",
    "            print(\"  step_dir:\", step_dir)\n",
    "\n",
    "        newton_decrement_square = - grad.T @ step_dir\n",
    "        if newton_decrement_square * 0.5 < eps:\n",
    "            if verbose:\n",
    "                print(\"Newton decrement below threshold\")\n",
    "            break\n",
    "\n",
    "        t = 1.\n",
    "        x_next = x\n",
    "        grad_norm = np.linalg.norm(grad)\n",
    "        while t >= t_min:\n",
    "            step = t * step_dir\n",
    "            x_next = x + step\n",
    "            obj_value = objective_fn(x_next)\n",
    "            dobj = obj_value - prev_obj_value\n",
    "            # Armijo-Goldstein inequality\n",
    "            if dobj <= - c * t * grad_norm:\n",
    "                break\n",
    "            t *= beta\n",
    "        x = x_next\n",
    "        step = step[..., np.newaxis]\n",
    "\n",
    "        # Update inverse Hessian approximation\n",
    "        y = (grad_fn(x) - grad)[..., np.newaxis]\n",
    "        B_inv_step1_nom = (step.T @ y + y.T @ B_inv @ y) * (step @ step.T)\n",
    "        B_inv_step1_denom = (step.T @ y) ** 2\n",
    "        B_inv_step1 = B_inv_step1_nom / B_inv_step1_denom\n",
    "        B_inv_step2 = - (B_inv @ y @ step.T + step @ y.T @ B_inv) / (step.T @ y)\n",
    "        B_inv += B_inv_step1 + B_inv_step2\n",
    "\n",
    "        iteration += 1\n",
    "    info = {\n",
    "        \"obj_value\": obj_value,\n",
    "        \"dobj\": dobj,\n",
    "        \"obj_tol\": obj_tol,\n",
    "        \"iteration\": iteration,\n",
    "        \"max_iterations\": max_iterations,\n",
    "        \"grad\": grad,\n",
    "        \"t\": t,\n",
    "        \"step_dir\": step_dir,\n",
    "        \"step\": step,\n",
    "        \"B_inv\": B_inv,\n",
    "    }\n",
    "    return x, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_barrier_method(obj_fn, grad_fn, hessian_fn,\n",
    "                       constraints, grad_constraints, hessian_constraints,\n",
    "                       x0, max_iterations=1000, t0=0.01, t_factor=10., eps=1e-12, cnst_tol=1e-300,\n",
    "                       use_bfgs=True, verbose=False):\n",
    "\n",
    "    if len(constraints) == 0:\n",
    "        if use_bfgs:\n",
    "            return min_bfgs(obj_fn, grad_fn, x0, max_iterations=max_iterations)\n",
    "        else:\n",
    "            return min_newton_method(obj_fn, grad_fn, hessian_fn, x0, max_iterations=max_iterations)\n",
    "\n",
    "    for i, constraint_fn in enumerate(constraints):\n",
    "        cnst = constraint_fn(x0)\n",
    "        assert cnst <= -cnst_tol, \\\n",
    "            \"The initial point has to be feasible for constraint {} ({} is above tolerance {})\".format(\n",
    "                i, np.abs(cnst), cnst_tol)\n",
    "\n",
    "    def fun_with_barrier(x, t):\n",
    "        barrier = 0\n",
    "        for constraint_fn in constraints:\n",
    "            cnst = constraint_fn(x)\n",
    "            if cnst >= -cnst_tol:\n",
    "                return float(\"inf\")\n",
    "            barrier += - np.log(-cnst)\n",
    "        y = obj_fn(x) + 1 / t * barrier\n",
    "        return y\n",
    "\n",
    "    def grad_with_barrier(x, t):\n",
    "        barrier_grad = 0\n",
    "        for constraint_fn, grad_constraint_fn in zip(constraints, grad_constraints):\n",
    "            cnst = constraint_fn(x)\n",
    "            grad_cnst = grad_constraint_fn(x)\n",
    "            barrier_grad += - (1 / cnst) * grad_cnst\n",
    "        grad = grad_fn(x) + 1 / t * barrier_grad\n",
    "        return grad\n",
    "\n",
    "    if not use_bfgs:\n",
    "        def hessian_with_barrier(x, t):\n",
    "            barrier_hessian = 0\n",
    "            for constraint_fn, grad_constraint_fn, hessian_constraint_fn \\\n",
    "            in zip(constraints, grad_constraints, hessian_constraints):\n",
    "                cnst = constraint_fn(x)\n",
    "                grad_cnst = grad_constraint_fn(x)\n",
    "                hessian_cnst = hessian_constraint_fn(x)\n",
    "                barrier_hessian += (1 / cnst**2) * grad_cnst[..., np.newaxis] @ grad_cnst[..., np.newaxis].T\n",
    "                barrier_hessian += - (1 / cnst**2) * hessian_cnst\n",
    "            hessian = hessian_fn(x) + 1 / t * barrier_hessian\n",
    "            return hessian\n",
    "\n",
    "    x = x0\n",
    "    t = t0\n",
    "    m = len(constraints)\n",
    "    outer_iteration = 0\n",
    "    info = {}\n",
    "\n",
    "    while m / t >= eps:\n",
    "        if use_bfgs:\n",
    "            x_min, new_info = min_bfgs(\n",
    "                lambda x: fun_with_barrier(x, t),\n",
    "                lambda x: grad_with_barrier(x, t),\n",
    "                x, max_iterations=max_iterations, verbose=verbose)\n",
    "        else:\n",
    "            x_min, new_info = min_newton_method(\n",
    "                lambda x: fun_with_barrier(x, t),\n",
    "                lambda x: grad_with_barrier(x, t),\n",
    "                lambda x: hessian_with_barrier(x, t),\n",
    "                x, max_iterations=max_iterations, verbose=verbose)\n",
    "\n",
    "        if not np.isfinite(new_info['obj_value']):\n",
    "            if verbose:\n",
    "                print(\"Objective value diverged. Stopping.\")\n",
    "            break\n",
    "        if verbose:\n",
    "            print(\"x_min:\", x_min)\n",
    "        x = x_min\n",
    "        info = new_info\n",
    "        t *= t_factor\n",
    "        outer_iteration += 1\n",
    "\n",
    "    info[\"outer_iteration\"] = outer_iteration\n",
    "    info[\"t_barrier\"] = t\n",
    "    return x, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.array([-200., 5.])\n",
    "max_iterations = 100\n",
    "x_min, info = min_barrier_method(\n",
    "    objective_fn3, grad_fn3, hessian_fn3,\n",
    "    constraints3, grad_constraints3, hessian_constraints3,\n",
    "    x0, max_iterations=max_iterations, verbose=False)\n",
    "\n",
    "print(\"minimum x: {}\".format(x_min))\n",
    "print(\"objective value: {}\".format(info[\"obj_value\"]))\n",
    "print(\"constraint values: {}\".format([constraint_fn(x_min) for constraint_fn in constraints2]))\n",
    "print(\"iteration: {}\".format(info[\"iteration\"]))\n",
    "print(\"outer iterations: {}\".format(info[\"outer_iteration\"]))\n",
    "\n",
    "# Plot function and found minimum\n",
    "plt.figure()\n",
    "x = np.linspace(-np.abs(x_min[0]), +np.abs(x_min[0]), 100)[:, np.newaxis]\n",
    "plt.plot(x, [objective_fn3(np.array([v, x_min[1]])) for v in x])\n",
    "plt.plot([x_min[0]], [objective_fn3(x_min)], 'rx')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "x = np.linspace(-np.abs(x_min[1]), +np.abs(x_min[1]), 100)[:, np.newaxis]\n",
    "plt.plot(x, [objective_fn3(np.array([x_min[0], v])) for v in x])\n",
    "plt.plot([x_min[1]], [objective_fn3(x_min)], 'rx')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.array([0.5])\n",
    "max_iterations = 100\n",
    "x_min, info = min_barrier_method(\n",
    "    objective_fn2, grad_fn2, hessian_fn2,\n",
    "    constraints2, grad_constraints2, hessian_constraints2,\n",
    "    x0, t0=10, max_iterations=max_iterations, verbose=False)\n",
    "\n",
    "print(\"minimum x: {}\".format(x_min))\n",
    "print(\"objective value: {}\".format(info[\"obj_value\"]))\n",
    "print(\"constraint values: {}\".format([constraint_fn(x_min) for constraint_fn in constraints2]))\n",
    "print(\"iteration: {}\".format(info[\"iteration\"]))\n",
    "\n",
    "# Plot function and found minimum\n",
    "plt.figure()\n",
    "x = np.linspace(-np.abs(x_min[0]), +np.abs(x_min[0]), 100)[:, np.newaxis]\n",
    "plt.plot(x, objective_fn2(x))\n",
    "plt.plot([x_min[0]], [objective_fn2(x_min)], 'rx')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
