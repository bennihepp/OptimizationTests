{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods for non-linear least-squares with manifold constraint using tangent space:\n",
    "- Gauss-newton\n",
    "- Levenberg-Marquardt\n",
    "\n",
    "### Examples for non-linear least-squares on manifolds:\n",
    "- Rotation in 2D (SO(2))\n",
    "- Rotation in 3D (SO(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import grad as auto_grad, jacobian as auto_jacobian\n",
    "import scipy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_square_with_qr(A, b):\n",
    "    \"\"\"Solve system of equations with square (real values) matrix A: A * x = b\"\"\"\"\"\n",
    "    Q, R = np.linalg.qr(A)\n",
    "    x = scipy.linalg.solve_triangular(R, Q.T @ b)\n",
    "    return x\n",
    "\n",
    "\n",
    "def solve_symm_with_chol(A, b):\n",
    "    \"\"\"Solve system of equations with symmetric (hermitian) matrix A: A * x = b\"\"\"\"\"\n",
    "    c, lower = scipy.linalg.cho_factor(A)\n",
    "    x = scipy.linalg.cho_solve((c, lower), b)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(41)\n",
    "\n",
    "\"\"\"Example for optimization on SO(2) manifold\"\"\"\n",
    "\n",
    "def skew_symmetric_3(x):\n",
    "    return np.array([[    0, -x[2],  x[1]],\n",
    "                     [ x[2],     0, -x[0]],\n",
    "                     [-x[1],  x[0],     0]])\n",
    "\n",
    "# Create test data (point rotated by 25 degree)\n",
    "p0 = np.array([1., 0.])[..., np.newaxis]\n",
    "angle0 = 25. * np.pi / 180.\n",
    "R_true = np.array([[np.cos(angle0), -np.sin(angle0)],\n",
    "                   [np.sin(angle0),  np.cos(angle0)]])\n",
    "p1 = np.dot(R_true, p0)\n",
    "# Introduce error so that non-manifold optimization leads to wrong result\n",
    "p1[1] += 0.1\n",
    "\n",
    "n0 = 4\n",
    "def residuals_fn0(x):\n",
    "    x = np.reshape(x, (2, 2))\n",
    "    p = np.dot(x, p0)\n",
    "    y = p - p1\n",
    "    return y.ravel()\n",
    "\n",
    "residuals_jacobian_fn0 = auto_jacobian(residuals_fn0)\n",
    "\n",
    "tangent_space_size_so2 = 1\n",
    "\n",
    "def exp_map_so2(eps):\n",
    "    R = np.array([[np.cos(eps[0]), -np.sin(eps[0])],\n",
    "                  [np.sin(eps[0]),  np.cos(eps[0])]])\n",
    "    return R\n",
    "\n",
    "def tangent_update_so2(x, eps):\n",
    "    R = exp_map_so2(eps)\n",
    "    x = np.reshape(x, (2, 2))\n",
    "    x = np.dot(R, x)\n",
    "    x = np.ravel(x)\n",
    "    return x\n",
    "\n",
    "def residuals_fn0_tangent_so2(x, eps):\n",
    "    R = exp_map_so2(eps)\n",
    "    x = np.reshape(x, (2, 2))\n",
    "    x = np.dot(R, x)\n",
    "    return residuals_fn0(x)\n",
    "\n",
    "tangent_residuals_jacobian_fn0 = auto_jacobian(residuals_fn0_tangent_so2, 1)\n",
    "# print(tangent_residuals_jacobian_fn0(R_true.ravel(), np.array([0.])))\n",
    "\n",
    "def tangent_residuals_jacobian_fn0(x, eps):\n",
    "    \"\"\"Define jacobian by hand\"\"\"\n",
    "    assert np.all(eps == 0)\n",
    "    x = np.reshape(x, (2, 2))\n",
    "    p = np.dot(x, p0)\n",
    "    q = np.array([[0., -1.],\n",
    "                  [1.,  0.]])\n",
    "    return np.dot(q, p)\n",
    "\n",
    "# print(tangent_residuals_jacobian_fn0(R_true.ravel(), np.array([0.])))\n",
    "\n",
    "\n",
    "\"\"\"Example for optimization on SO(3) manifold\"\"\"\n",
    "\n",
    "# Create test data (point rotated by 25 degree)\n",
    "p0_3d = np.array([0.6, 0.2, 0.3])[..., np.newaxis]\n",
    "angle0_3d = 25. * np.pi / 180.\n",
    "R_true_3d = np.array([[np.cos(angle0_3d), -np.sin(angle0_3d), 0.],\n",
    "                   [np.sin(angle0_3d),  np.cos(angle0_3d), 0.],\n",
    "                   [0.,              0.,             1.]])\n",
    "p1_3d = np.dot(R_true_3d, p0_3d)\n",
    "# Introduce error so that non-manifold optimization leads to wrong result\n",
    "p1_3d[1] += 0.05\n",
    "\n",
    "n0 = 9\n",
    "def residuals_fn1(x):\n",
    "    x = np.reshape(x, (3, 3))\n",
    "    p = np.dot(x, p0_3d)\n",
    "    y = p - p1_3d\n",
    "    return y.ravel()\n",
    "\n",
    "residuals_jacobian_fn1 = auto_jacobian(residuals_fn1)\n",
    "\n",
    "tangent_space_size_so3 = 3\n",
    "\n",
    "def exp_map_so3(eps):\n",
    "    omega = skew_symmetric_3(eps)\n",
    "    omega_sq = np.dot(omega, omega)\n",
    "    theta = np.linalg.norm(eps)\n",
    "    if theta < 1e-3:\n",
    "        R = np.eye(3) + omega + 0.5 * omega_sq\n",
    "    else:\n",
    "        R = np.eye(3) + omega * (np.sin(theta) / theta) + omega_sq * ((1 - np.cos(theta)) / theta**2)\n",
    "    return R\n",
    "\n",
    "def tangent_update_so3(x, eps):\n",
    "    R = exp_map_so3(eps)\n",
    "    x = np.reshape(x, (3, 3))\n",
    "    x = np.dot(R, x)\n",
    "    x = np.ravel(x)\n",
    "    return x\n",
    "\n",
    "def residuals_fn1_tangent_so3(x, eps):\n",
    "    R = exp_map_so3(eps)\n",
    "    x = np.reshape(x, (3, 3))\n",
    "    x = np.dot(R, x)\n",
    "    return residuals_fn1(x)\n",
    "\n",
    "tangent_residuals_jacobian_fn1 = auto_jacobian(residuals_fn1_tangent_so3, 1)\n",
    "#print(tangent_residuals_jacobian_fn1(R_true_3d.ravel(), np.array([0., 0., 0.])))\n",
    "\n",
    "def tangent_residuals_jacobian_fn1(x, eps):\n",
    "    \"\"\"Define jacobian by hand\"\"\"\n",
    "    assert np.all(eps == 0)\n",
    "    x = np.reshape(x, (3, 3))\n",
    "    p = np.dot(x, p0_3d).ravel()\n",
    "    return - skew_symmetric_3(p)\n",
    "\n",
    "#print(tangent_residuals_jacobian_fn1(R_true_3d.ravel(), np.array([0., 0., 0.])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_gauss_newton(residuals_fn, residuals_jacobian_fn, x0, obj_tol=1e-12, max_iterations=10**6):\n",
    "    x = np.array(x0)\n",
    "    iteration = 0\n",
    "    step_size = 1\n",
    "    dobj = float(\"inf\")\n",
    "    residuals = residuals_fn(x)\n",
    "    obj_value = np.sum(residuals**2)\n",
    "    while np.abs(dobj) > obj_tol and iteration < max_iterations:\n",
    "        grad = residuals_jacobian_fn(x)\n",
    "        pseudo_inv = np.linalg.pinv(grad.T @ grad)\n",
    "        step_dir = (pseudo_inv @ grad.T) @ residuals\n",
    "        prev_obj_value = obj_value\n",
    "        x = x - step_size * step_dir\n",
    "        residuals = residuals_fn(x)\n",
    "        obj_value = np.sum(residuals**2)\n",
    "        dobj = obj_value - prev_obj_value\n",
    "        iteration += 1\n",
    "    info = {\n",
    "        \"obj_value\": obj_value,\n",
    "        \"dobj\": dobj,\n",
    "        \"obj_tol\": obj_tol,\n",
    "        \"iteration\": iteration,\n",
    "        \"max_iterations\": max_iterations,\n",
    "        \"grad\": grad,\n",
    "        \"step_dir\": step_dir,\n",
    "        \"step_size\": step_size,\n",
    "    }\n",
    "    return x, info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_levenberg_marquardt(residuals_fn, residuals_jacobian_fn, x0, lambda0=1., v=2.,\n",
    "                            obj_tol=1e-12, max_iterations=10**6):\n",
    "    x = np.array(x0)\n",
    "    lambda_ = lambda0\n",
    "    iteration = 0\n",
    "    dobj = float(\"inf\")\n",
    "    residuals = residuals_fn(x)\n",
    "    obj_value = np.sum(residuals**2)\n",
    "    while np.abs(dobj) > obj_tol and iteration < max_iterations:\n",
    "        prev_obj_value = obj_value\n",
    "        grad = residuals_jacobian_fn(x)\n",
    "        grad_grad = grad.T @ grad\n",
    "        while True:  # Should max iteration criterion here\n",
    "            lambda1 = lambda_\n",
    "            lambda2 = lambda_ / v\n",
    "            H1 = grad_grad + lambda1 * np.diag(np.diag(grad_grad))\n",
    "            H2 = grad_grad + lambda2 * np.diag(np.diag(grad_grad))\n",
    "            pseudo_inv1 = np.linalg.pinv(H1)\n",
    "            pseudo_inv2 = np.linalg.pinv(H2)\n",
    "            step1 = (pseudo_inv1 @ grad.T) @ residuals\n",
    "            step2 = (pseudo_inv2 @ grad.T) @ residuals\n",
    "            x_next1 = x - step1\n",
    "            x_next2 = x - step2\n",
    "            residuals1 = residuals_fn(x_next1)\n",
    "            residuals2 = residuals_fn(x_next2)\n",
    "            obj_value1 = np.sum(residuals1**2)\n",
    "            obj_value2 = np.sum(residuals2**2)\n",
    "            dobj1 = obj_value1 - prev_obj_value\n",
    "            dobj2 = obj_value2 - prev_obj_value\n",
    "            if dobj1 > 0 and dobj2 > 0:\n",
    "                lambda_ *= v\n",
    "            elif dobj2 < 0:\n",
    "                lambda_ = lambda2\n",
    "                x = x_next2\n",
    "                step = step2\n",
    "                residuals = residuals2\n",
    "                obj_value = obj_value2\n",
    "                dobj = dobj2\n",
    "                break\n",
    "            else:\n",
    "                x = x_next1\n",
    "                step = step1\n",
    "                residuals = residuals1\n",
    "                obj_value = obj_value1\n",
    "                dobj = dobj1\n",
    "                break\n",
    "        iteration += 1\n",
    "    info = {\n",
    "        \"obj_value\": obj_value,\n",
    "        \"dobj\": dobj,\n",
    "        \"obj_tol\": obj_tol,\n",
    "        \"iteration\": iteration,\n",
    "        \"max_iterations\": max_iterations,\n",
    "        \"grad\": grad,\n",
    "        \"step\": step,\n",
    "    }\n",
    "    return x, info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_gauss_newton_tangent_space(residuals_fn,\n",
    "                                   tangent_update_fn,\n",
    "                                   tangent_jacobian_fn,\n",
    "                                   tangent_space_size,\n",
    "                                   x0, obj_tol=1e-12, max_iterations=10**6):\n",
    "    eps0 = np.zeros((tangent_space_size,))\n",
    "    x = np.array(x0)\n",
    "    iteration = 0\n",
    "    step_size = 1\n",
    "    dobj = float(\"inf\")\n",
    "    residuals = residuals_fn(x)\n",
    "    obj_value = np.sum(residuals**2)\n",
    "    while np.abs(dobj) > obj_tol and iteration < max_iterations:\n",
    "        grad = tangent_jacobian_fn(x, eps0)\n",
    "        pseudo_inv = np.linalg.pinv(grad.T @ grad)\n",
    "        step_dir = (pseudo_inv @ grad.T) @ residuals\n",
    "        prev_obj_value = obj_value\n",
    "        x = tangent_update_fn(x, - step_size * step_dir)\n",
    "        residuals = residuals_fn(x)\n",
    "        obj_value = np.sum(residuals**2)\n",
    "        dobj = obj_value - prev_obj_value\n",
    "        iteration += 1\n",
    "    info = {\n",
    "        \"obj_value\": obj_value,\n",
    "        \"dobj\": dobj,\n",
    "        \"obj_tol\": obj_tol,\n",
    "        \"iteration\": iteration,\n",
    "        \"max_iterations\": max_iterations,\n",
    "        \"grad\": grad,\n",
    "        \"step_dir\": step_dir,\n",
    "        \"step_size\": step_size,\n",
    "    }\n",
    "    return x, info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_levenberg_marquardt_tangent_space(residuals_fn,\n",
    "                                          tangent_update_fn,\n",
    "                                          tangent_jacobian_fn,\n",
    "                                          tangent_space_size,\n",
    "                                          x0, lambda0=1., v=2.,\n",
    "                                          obj_tol=1e-12, max_iterations=10**6):\n",
    "    eps0 = np.zeros((tangent_space_size,))\n",
    "    x = np.array(x0)\n",
    "    lambda_ = lambda0\n",
    "    iteration = 0\n",
    "    dobj = float(\"inf\")\n",
    "    residuals = residuals_fn(x)\n",
    "    obj_value = np.sum(residuals**2)\n",
    "    while np.abs(dobj) > obj_tol and iteration < max_iterations:\n",
    "        prev_obj_value = obj_value\n",
    "        grad = tangent_jacobian_fn(x, eps0)\n",
    "        grad_grad = grad.T @ grad\n",
    "        while True:  # Should max iteration criterion here\n",
    "            lambda1 = lambda_\n",
    "            lambda2 = lambda_ / v\n",
    "            H1 = grad_grad + lambda1 * np.diag(np.diag(grad_grad))\n",
    "            H2 = grad_grad + lambda2 * np.diag(np.diag(grad_grad))\n",
    "            pseudo_inv1 = np.linalg.pinv(H1)\n",
    "            pseudo_inv2 = np.linalg.pinv(H2)\n",
    "            step1 = (pseudo_inv1 @ grad.T) @ residuals\n",
    "            step2 = (pseudo_inv2 @ grad.T) @ residuals\n",
    "            x_next1 = tangent_update_fn(x, - step1)\n",
    "            x_next2 = tangent_update_fn(x, - step2)\n",
    "            residuals1 = residuals_fn(x_next1)\n",
    "            residuals2 = residuals_fn(x_next2)\n",
    "            obj_value1 = np.sum(residuals1**2)\n",
    "            obj_value2 = np.sum(residuals2**2)\n",
    "            dobj1 = obj_value1 - prev_obj_value\n",
    "            dobj2 = obj_value2 - prev_obj_value\n",
    "            if dobj1 > 0 and dobj2 > 0:\n",
    "                lambda_ *= v\n",
    "            elif dobj2 < 0:\n",
    "                lambda_ = lambda2\n",
    "                x = x_next2\n",
    "                step = step2\n",
    "                residuals = residuals2\n",
    "                obj_value = obj_value2\n",
    "                dobj = dobj2\n",
    "                break\n",
    "            else:\n",
    "                x = x_next1\n",
    "                step = step1\n",
    "                residuals = residuals1\n",
    "                obj_value = obj_value1\n",
    "                dobj = dobj1\n",
    "                break\n",
    "        iteration += 1\n",
    "    info = {\n",
    "        \"obj_value\": obj_value,\n",
    "        \"dobj\": dobj,\n",
    "        \"obj_tol\": obj_tol,\n",
    "        \"iteration\": iteration,\n",
    "        \"max_iterations\": max_iterations,\n",
    "        \"grad\": grad,\n",
    "        \"step\": step,\n",
    "    }\n",
    "    return x, info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum x: [[0.90630779 0.        ]\n",
      " [0.52261826 1.        ]]\n",
      "objective value: 0.0\n",
      "error: 0.0\n",
      "determinant of R_true: 0.9999999999999999\n",
      "determinant of x_min: 0.9063077870366499\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Minimize function #0 with gauss newton without manifold constraint\"\"\"\n",
    "\n",
    "x0 = np.eye(2).ravel()\n",
    "# x0 = np.zeros((2, 2)).ravel()\n",
    "\n",
    "max_iterations = 100\n",
    "x_min, info = min_gauss_newton(residuals_fn0, residuals_jacobian_fn0, x0, max_iterations=max_iterations)\n",
    "x_min = np.reshape(x_min, (2, 2))\n",
    "print(\"minimum x: {}\".format(x_min))\n",
    "print(\"objective value: {}\".format(info[\"obj_value\"]))\n",
    "\n",
    "p = np.dot(x_min, p0)\n",
    "err = np.sum((p - p1)**2)\n",
    "print(\"error: {}\".format(err))\n",
    "\n",
    "print(\"determinant of R_true: {}\".format(np.linalg.det(R_true)))\n",
    "print(\"determinant of x_min: {}\".format(np.linalg.det(x_min)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum x: [[ 0.86628965 -0.49954204]\n",
      " [ 0.49954204  0.86628965]]\n",
      "objective value: 0.0021339636446826\n",
      "error: 0.0021339636446826\n",
      "determinant of R_true: 0.9999999999999999\n",
      "determinant of x_min: 1.0000000000000002\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Minimize function #0 with gauss newton with manifold constraint (using tangent space)\"\"\"\n",
    "\n",
    "x0 = np.eye(2).ravel()\n",
    "\n",
    "max_iterations = 100\n",
    "# print(residuals_fn4(x0).shape)\n",
    "# print(residuals_jacobian_fn4(x0).shape)\n",
    "x_min, info = min_gauss_newton_tangent_space(\n",
    "    residuals_fn0,\n",
    "    tangent_update_so2,\n",
    "    tangent_residuals_jacobian_fn0,\n",
    "    tangent_space_size_so2,\n",
    "    x0, max_iterations=max_iterations)\n",
    "x_min = np.reshape(x_min, (2, 2))\n",
    "print(\"minimum x: {}\".format(x_min))\n",
    "print(\"objective value: {}\".format(info[\"obj_value\"]))\n",
    "\n",
    "p = np.dot(x_min, p0)\n",
    "err = np.sum((p - p1)**2)\n",
    "print(\"error: {}\".format(err))\n",
    "\n",
    "print(\"determinant of R_true: {}\".format(np.linalg.det(R_true)))\n",
    "print(\"determinant of x_min: {}\".format(np.linalg.det(x_min)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum x: [[ 0.86628968 -0.49954199]\n",
      " [ 0.49954199  0.86628968]]\n",
      "objective value: 0.00213396364468073\n",
      "error: 0.00213396364468073\n",
      "determinant of R_true: 0.9999999999999999\n",
      "determinant of x_min: 1.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Minimize function #0 with levenberg marquardt with manifold constraint (using tangent space)\"\"\"\n",
    "\n",
    "x0 = np.eye(2).ravel()\n",
    "\n",
    "max_iterations = 100\n",
    "# print(residuals_fn4(x0).shape)\n",
    "# print(residuals_jacobian_fn4(x0).shape)\n",
    "x_min, info = min_levenberg_marquardt_tangent_space(\n",
    "    residuals_fn0,\n",
    "    tangent_update_so2,\n",
    "    tangent_residuals_jacobian_fn0,\n",
    "    tangent_space_size_so2,\n",
    "    x0, max_iterations=max_iterations)\n",
    "x_min = np.reshape(x_min, (2, 2))\n",
    "print(\"minimum x: {}\".format(x_min))\n",
    "print(\"objective value: {}\".format(info[\"obj_value\"]))\n",
    "\n",
    "p = np.dot(x_min, p0)\n",
    "err = np.sum((p - p1)**2)\n",
    "print(\"error: {}\".format(err))\n",
    "\n",
    "print(\"determinant of R_true: {}\".format(np.linalg.det(R_true)))\n",
    "print(\"determinant of x_min: {}\".format(np.linalg.det(x_min)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum x: [[ 0.82766655 -0.05744448 -0.08616672]\n",
      " [ 0.34877451  1.11625817  0.17438725]\n",
      " [ 0.          0.          1.        ]]\n",
      "objective value: 0.0\n",
      "error: 0.0\n",
      "determinant of R_true_3d: 0.9999999999999999\n",
      "determinant of x_min: 0.9439247241115509\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Minimize function #1 with gauss newton without manifold constraint\"\"\"\n",
    "\n",
    "x0 = np.eye(3).ravel()\n",
    "# x0 = np.zeros((3, 3)).ravel()\n",
    "\n",
    "max_iterations = 100\n",
    "x_min, info = min_gauss_newton(residuals_fn1, residuals_jacobian_fn1, x0, max_iterations=max_iterations)\n",
    "x_min = np.reshape(x_min, (3, 3))\n",
    "print(\"minimum x: {}\".format(x_min))\n",
    "print(\"objective value: {}\".format(info[\"obj_value\"]))\n",
    "\n",
    "p = np.dot(x_min, p0_3d)\n",
    "err = np.sum((p - p1_3d)**2)\n",
    "print(\"error: {}\".format(err))\n",
    "\n",
    "print(\"determinant of R_true_3d: {}\".format(np.linalg.det(R_true_3d)))\n",
    "print(\"determinant of x_min: {}\".format(np.linalg.det(x_min)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum x: [[ 0.91713928 -0.38118256 -0.11642761]\n",
      " [ 0.39562361  0.90609388  0.14991946]\n",
      " [ 0.04834766 -0.18355854  0.98181911]]\n",
      "objective value: 0.0010309731255731854\n",
      "error: 0.0010309731255731854\n",
      "determinant of R_true_3d: 0.9999999999999999\n",
      "determinant of x_min: 1.000000000000001\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Minimize function #1 with gauss newton with manifold constraint (using tangent space)\"\"\"\n",
    "\n",
    "x0 = np.eye(3).ravel()\n",
    "\n",
    "max_iterations = 100\n",
    "# print(residuals_fn4(x0).shape)\n",
    "# print(residuals_jacobian_fn4(x0).shape)\n",
    "x_min, info = min_gauss_newton_tangent_space(\n",
    "    residuals_fn1,\n",
    "    tangent_update_so3,\n",
    "    tangent_residuals_jacobian_fn1,\n",
    "    tangent_space_size_so3,\n",
    "    x0, max_iterations=max_iterations)\n",
    "x_min = np.reshape(x_min, (3, 3))\n",
    "print(\"minimum x: {}\".format(x_min))\n",
    "print(\"objective value: {}\".format(info[\"obj_value\"]))\n",
    "\n",
    "p = np.dot(x_min, p0_3d)\n",
    "err = np.sum((p - p1_3d)**2)\n",
    "print(\"error: {}\".format(err))\n",
    "\n",
    "print(\"determinant of R_true_3d: {}\".format(np.linalg.det(R_true_3d)))\n",
    "print(\"determinant of x_min: {}\".format(np.linalg.det(x_min)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum x: [[ 0.93298448 -0.29580328 -0.20503749]\n",
      " [ 0.34294071  0.9035157   0.257004  ]\n",
      " [ 0.10923197 -0.31009645  0.94440911]]\n",
      "objective value: 0.0010309731255715919\n",
      "error: 0.0010309731255715919\n",
      "determinant of R_true_3d: 0.9999999999999999\n",
      "determinant of x_min: 1.0000000000000615\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Minimize function #1 with levenberg marquardt newton with manifold constraint (using tangent space)\"\"\"\n",
    "\n",
    "x0 = np.eye(3).ravel()\n",
    "\n",
    "max_iterations = 100\n",
    "# print(residuals_fn4(x0).shape)\n",
    "# print(residuals_jacobian_fn4(x0).shape)\n",
    "x_min, info = min_levenberg_marquardt_tangent_space(\n",
    "    residuals_fn1,\n",
    "    tangent_update_so3,\n",
    "    tangent_residuals_jacobian_fn1,\n",
    "    tangent_space_size_so3,\n",
    "    x0, max_iterations=max_iterations)\n",
    "x_min = np.reshape(x_min, (3, 3))\n",
    "print(\"minimum x: {}\".format(x_min))\n",
    "print(\"objective value: {}\".format(info[\"obj_value\"]))\n",
    "\n",
    "p = np.dot(x_min, p0_3d)\n",
    "err = np.sum((p - p1_3d)**2)\n",
    "print(\"error: {}\".format(err))\n",
    "\n",
    "print(\"determinant of R_true_3d: {}\".format(np.linalg.det(R_true_3d)))\n",
    "print(\"determinant of x_min: {}\".format(np.linalg.det(x_min)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-3.6-optim",
   "language": "python",
   "name": "python-3.6-optim"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
